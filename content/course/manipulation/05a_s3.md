---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Lire des données depuis s3"
date: 2020-07-09T15:00:00Z
draft: false
weight: 70
slug: readS3
type: book
tags:
  - S3
  - boto3
categories:
  - Tutoriel
summary: |
  Dans les entreprises et administrations, un nombre croissant 
  d'infrastructure se basent sur des Cloud, qui sont des sessions 
  non persistentes où les données ne sont pas stockées dans les mêmes
  serveurs que les machines qui exécutent du code. L'une des technologies
  dominantes dans le domaine est un système de stockage nommé S3,
  développé par Amazon. 
  Python, à travers les packages boto3 et s3fs,
  permet d'utiliser ce système de stockage distant comme si on
  accédait à des fichiers depuis son poste personnel. 
---



<a href="https://github.com/linogaliana/python-datascientist/blob/master/C:/Users/W3CRK9/Documents/FORMATIONS/python-datascientist/notebooks/course/manipulation/05a_s3.ipynb" class="github"><i class="fab fa-github"></i></a>
[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master/C:/Users/W3CRK9/Documents/FORMATIONS/python-datascientist/notebooks/course/manipulation/05a_s3.ipynb)
[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master/C:/Users/W3CRK9/Documents/FORMATIONS/python-datascientist/notebooks/course/manipulation/05a_s3.ipynb)
[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?autoLaunch=true&onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABmanipulation%2005a_s3%C2%BB&security.allowlist.enabled=false)
<br>
[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFkAAABZCAMAAABi1XidAAAB8lBMVEX///9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olL1olJXmsrmZYH1olL1olJXmsrmZYH1olL1olL0nFf1olJXmsrmZYH1olJXmsq8dZb1olJXmsrmZYH1olJXmspXmspXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olJXmsrmZYH1olL1olLeaIVXmsrmZYH1olL1olL1olJXmsrmZYH1olLna31Xmsr1olJXmsr1olJXmsrmZYH1olLqoVr1olJXmsr1olJXmsrmZYH1olL1olKkfaPobXvviGabgadXmsqThKuofKHmZ4Dobnr1olJXmsr1olJXmspXmsr1olJXmsrfZ4TuhWn1olL1olJXmsqBi7X1olJXmspZmslbmMhbmsdemsVfl8ZgmsNim8Jpk8F0m7R4m7F5nLB6jbh7jbiDirOEibOGnKaMhq+PnaCVg6qWg6qegKaff6WhnpKofKGtnomxeZy3noG6dZi+n3vCcpPDcpPGn3bLb4/Mb47UbIrVa4rYoGjdaIbeaIXhoWHmZYHobXvpcHjqdHXreHLroVrsfG/uhGnuh2bwj2Hxk17yl1vzmljzm1j0nlX1olL3AJXWAAAAbXRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hgYGBkcHBwcXl8gICAgoiIkJCQlJicnJ2goKCmqK+wsLC4usDAwMjP0NDQ1NbW3Nzg4ODi5+3v8PDw8/T09PX29vb39/f5+fr7+/z8/Pz9/v7+zczCxgAABC5JREFUeAHN1ul3k0UUBvCb1CTVpmpaitAGSLSpSuKCLWpbTKNJFGlcSMAFF63iUmRccNG6gLbuxkXU66JAUef/9LSpmXnyLr3T5AO/rzl5zj137p136BISy44fKJXuGN/d19PUfYeO67Znqtf2KH33Id1psXoFdW30sPZ1sMvs2D060AHqws4FHeJojLZqnw53cmfvg+XR8mC0OEjuxrXEkX5ydeVJLVIlV0e10PXk5k7dYeHu7Cj1j+49uKg7uLU61tGLw1lq27ugQYlclHC4bgv7VQ+TAyj5Zc/UjsPvs1sd5cWryWObtvWT2EPa4rtnWW3JkpjggEpbOsPr7F7EyNewtpBIslA7p43HCsnwooXTEc3UmPmCNn5lrqTJxy6nRmcavGZVt/3Da2pD5NHvsOHJCrdc1G2r3DITpU7yic7w/7Rxnjc0kt5GC4djiv2Sz3Fb2iEZg41/ddsFDoyuYrIkmFehz0HR2thPgQqMyQYb2OtB0WxsZ3BeG3+wpRb1vzl2UYBog8FfGhttFKjtAclnZYrRo9ryG9uG/FZQU4AEg8ZE9LjGMzTmqKXPLnlWVnIlQQTvxJf8ip7VgjZjyVPrjw1te5otM7RmP7xm+sK2Gv9I8Gi++BRbEkR9EBw8zRUcKxwp73xkaLiqQb+kGduJTNHG72zcW9LoJgqQxpP3/Tj//c3yB0tqzaml05/+orHLksVO+95kX7/7qgJvnjlrfr2Ggsyx0eoy9uPzN5SPd86aXggOsEKW2Prz7du3VID3/tzs/sSRs2w7ovVHKtjrX2pd7ZMlTxAYfBAL9jiDwfLkq55Tm7ifhMlTGPyCAs7RFRhn47JnlcB9RM5T97ASuZXIcVNuUDIndpDbdsfrqsOppeXl5Y+XVKdjFCTh+zGaVuj0d9zy05PPK3QzBamxdwtTCrzyg/2Rvf2EstUjordGwa/kx9mSJLr8mLLtCW8HHGJc2R5hS219IiF6PnTusOqcMl57gm0Z8kanKMAQg0qSyuZfn7zItsbGyO9QlnxY0eCuD1XL2ys/MsrQhltE7Ug0uFOzufJFE2PxBo/YAx8XPPdDwWN0MrDRYIZF0mSMKCNHgaIVFoBbNoLJ7tEQDKxGF0kcLQimojCZopv0OkNOyWCCg9XMVAi7ARJzQdM2QUh0gmBozjc3Skg6dSBRqDGYSUOu66Zg+I2fNZs/M3/f/Grl/XnyF1Gw3VKCez0PN5IUfFLqvgUN4C0qNqYs5YhPL+aVZYDE4IpUk57oSFnJm4FyCqqOE0jhY2SMyLFoo56zyo6becOS5UVDdj7Vih0zp+tcMhwRpBeLyqtIjlJKAIZSbI8SGSF3k0pA3mR5tHuwPFoa7N7reoq2bqCsAk1HqCu5uvI1n6JuRXI+S1Mco54YmYTwcn6Aeic+kssXi8XpXC4V3t7/ADuTNKaQJdScAAAAAElFTkSuQmCC)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=C:/Users/W3CRK9/Documents/FORMATIONS/python-datascientist/notebooks/course/manipulation/05a_s3.ipynb)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master/C:/Users/W3CRK9/Documents/FORMATIONS/python-datascientist/notebooks/course/manipulation/05a_s3.ipynb)
[![githubdev](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/linogaliana/python-datascientist/C:/Users/W3CRK9/Documents/FORMATIONS/python-datascientist/notebooks/course/manipulation/05a_s3.ipynb)

## Qu'est-ce que le système de stockage S3 ?

Dans les entreprises et administrations, un nombre croissant de données sont
disponibles depuis un système de stockage
nommé `S3`. 
Le système S3 (*Simple Storage System*) est un système de stockage développé
par Amazon et qui est maintenant devenu une référence pour le stockage en ligne.
Il s'agit d'une architecture à la fois
sécurisée (données cryptées, accès restreints) et performante.

Le concept central du système S3 est le *bucket*.
Un *bucket* est un espace (privé ou partagé) où on peut stocker une
arborescence de fichiers. Pour accéder aux fichiers figurant
dans un *bucket* privé, il fournit des jetons d'accès (l'équivalent d'un mot de passe)
reconnus par le serveur de stockage. On peut alors lire et écrire dans le *bucket*.

Le SSP cloud 
[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?onyxia.friendlyName=%C2%ABpython-datascientist%C2%BB&resources.requests.memory=%C2%AB4Gi%C2%BB) repose par exemple sur cette infrastructure technique. 


## Comment faire avec Python ?

### Les librairies principales

L'interaction entre ce système distant de fichiers et une session locale de Python
est possible grâce à des API. Les deux principales librairies sont les suivantes:

* [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), une librairie créée et maintenue par Amazon ;
* [s3fs](https://s3fs.readthedocs.io/en/latest/), une librairie qui permet d'interagir avec les fichiers stockés à l'instar d'un filesystem classique.


Ces deux librairies offrent toutes deux la possibilité de se connecter depuis `Python`,
à un dépôt de fichiers distant, de lister les fichiers disponibles dans un
*bucket*, d'en télécharger un ou plusieurs ou de faire de l'*upload*

{{% box status="note" title="Note" icon="fa fa-comment" %}}
Les exemples suivants seront réplicables pour les utilisateurs de la plateforme
SSP-cloud
{{% /box %}}

Personnellement, j'ai une petite préférence pour `boto3` que je trouve plus
intuitif.

### Quelques cas spéciaux

La librairie ultra-performante [`arrow`](https://arrow.apache.org/docs/python/)
propose également un accès à des fichiers sur 
S3 comme s'ils étaient disponibles sur un _filesystem_
local. Un exemple, assez court, est disponible 
[dans la documentation officielle](https://arrow.apache.org/docs/python/filesystems.html#s3)

Les utilisateurs de
[`snakemake`](https://snakemake.readthedocs.io/en/stable/snakefiles/remote_files.html)
peuvent également 
lire ou écrire des tables intermédiaires sur `S3`
comme s'ils utilisaient un
système de fichier local. En arrière-plan, `snakemake`
va utiliser `boto3` pour communiquer avec le système
de stockage


## Connexion à un bucket

Par la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent
à s'authentifier. 


```python
key_id = 'MY_KEY_ID'
access_key = 'MY_ACCESS_KEY'
token = "MY_TOKEN"
```

Ces valeurs peuvent être également disponibles dans 
les variables d'environnement de `Python`. Comme il s'agit d'une information
d'authentification personnelle, il ne faut pas stocker les vraies valeurs de ces
variables dans un projet, sous peine de partager des traits d'identité sans le
vouloir lors d'un partage de code. 

{{< tabs tabTotal="4" >}}
{{% tab tabName="boto3" %}}

Avec `boto3`, on créé d'abord un client puis on exécute des requêtes dessus.
Pour initialiser un client, il suffit, en supposant que l'url du dépôt S3 est
`"https://minio.lab.sspcloud.fr"`, de faire:


```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
```


{{% /tab %}}
{{% tab tabName="S3FS" %}}

La logique est identique avec `s3fs`. 

Si on a des jetons d'accès à jour et dans les variables d'environnement
adéquates:


```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
```


{{% /tab %}}
{{% tab tabName="Arrow" %}}

La logique d'`Arrow` est proche de celle de `s3fs`. Seuls les noms
d'arguments changent

Si on a des jetons d'accès à jour et dans les variables d'environnement
adéquates:


```python
from pyarrow import fs
s3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')
s3.get_file_info(fs.FileSelector('projet-relevanc', recursive=True))
```

{{% /tab %}}

{{% tab tabName="Snakemake" %}}

La logique de `Snakemake` est, quant à elle,
plus proche de celle de `boto3`. Seuls les noms
d'arguments changent

Si on a des jetons d'accès à jour et dans les variables d'environnement
adéquates:


```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
```

{{% /tab %}}
{{< /tabs >}}



Il se peut que la connexion à ce stade soit refusée (`HTTP error 403`).
Cela peut provenir 
d'une erreur dans l'URL utilisé. Cependant, cela reflète plus généralement
des paramètres d'authentification erronés.


{{< tabs tabTotal="3" >}}
{{% tab tabName="boto3" %}}

Les paramètres d'authentification sont des arguments supplémentaires:


```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr",
                  aws_access_key_id=key_id, 
                  aws_secret_access_key=access_key, 
                  aws_session_token = token)
```

{{% /tab %}}

{{% tab tabName="S3FS" %}}

La logique est la même, seuls les noms d'arguments diffèrent


```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},
  key = key_id, secret = access_key,
  token = token)
```

{{% /tab %}}

{{% tab tabName="Snakemake" %}}

La logique est la même, seuls les noms d'arguments diffèrent


```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)
```

{{% /tab %}}


{{< /tabs >}}


{{% box status="note" title="Notes" icon="fa fa-comment" %}}
Dans le SSP-cloud, 
lorsque l'initialisation du service `Jupyter` du SSP-cloud est récente
(moins de 12 heures), il est possible d'utiliser
automatiquement les jetons stockés automatiquement à la création du dépôt. 

Si on désire accéder aux données du SSP-cloud depuis une session python du
datalab (service VSCode, Jupyter...),
il faut remplacer l'url par `http://minio.lab.sspcloud.fr`
{{% /box %}}

## Lister les fichiers

S'il n'y a pas d'erreur à ce stade, c'est que la connexion est bien effective.
Pour le vérifier, on peut essayer de faire la liste des fichiers disponibles
dans un `bucket` auquel on désire accéder.

Par exemple, on peut vouloir
tester l'accès aux bases `FILOSOFI` (données de revenu localisées disponibles
sur <https://www.insee.fr>) au sein du bucket `donnees-insee`. 

{{< tabs tabTotal="4" >}}
{{% tab tabName="boto3" %}}

Pour cela,
la méthode `list_objects` offre toutes les options nécessaires:


```python
for key in s3.list_objects(Bucket='donnees-insee', Prefix='FILOSOFI')['Contents']:
    print(key['Key'])
```

{{% /tab %}}

{{% tab tabName="S3FS" %}}

Pour lister les fichiers, c'est la méthode `ls` (celle-ci ne liste pas par
défaut les fichiers de manière récursive comme `boto3`):


```python
fs.ls("donnees-insee")
```


{{% /tab %}}

{{% tab tabName="Arrow" %}}


```python
from pyarrow import fs
s3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')
s3.get_file_info(fs.FileSelector('donnees-insee', recursive=True))
```

{{% /tab %}}

{{< /tabs >}}


## Télécharger un fichier depuis s3 pour l'enregistrer en local

Cette méthode n'est en général pas recommandée car, comme on va le voir
par la suite, il est possible de lire à la volée des fichiers. Cependant,
télécharger un fichier depuis le _cloud_ pour l'écrire sur le disque
local peut parfois être utile. Par exemple, lorsqu'il est nécessaire
de dézipper un fichier. 

{{< tabs tabTotal="3" >}}
{{% tab tabName="boto3" %}}


```python
s3.download_file('donnees-insee', "FILOSOFI/2014/FILOSOFI_COM.csv", 'data.csv')
```

{{% /tab %}}

{{% tab tabName="S3FS" %}}

__TO BE COMPLETED__

{{% /tab %}}

{{% tab tabName="Snakemake" %}}


```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier = S3.remote(f'{bucket}/moninput.csv')
    output:
        fichier='mon_dossier_local/monoutput.csv'
    run:
        import pandas as pd
        df = pd.read_csv(input.fichier)
        df.to_csv(output.fichier)
```

{{% /tab %}}

{{< /tabs >}}


## Lire un fichier directement

La méthode précédente n'est pas optimale. En effet, l'un des intérêts des API
est qu'on peut traiter un fichier sur `s3` comme s'il s'agissait d'un fichier
sur son PC. Cela est d'ailleurs une manière plus sécurisée de procéder puisqu'on
lit les données à la volée, sans les écrire dans un filesystem local. 

{{< tabs tabTotal="3" >}}
{{% tab tabName="boto3" %}}


```python
obj = s3.get_object(Bucket='donnees-insee', Key="FILOSOFI/2014/FILOSOFI_COM.csv")
df = pd.read_csv(obj['Body'], sep = ";")
df.head(2)
```

{{% /tab %}}

{{% tab tabName="S3FS" %}}

Le code suivant devrait permettre d'effectuer la même opération avec `s3fs`


```python
df = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', "FILOSOFI/2014/FILOSOFI_COM.csv"),
                         mode='rb')
                 )

df.head(2)
```

{{% /tab %}}

{{% tab tabName="Arrow" %}}

`Arrow` est une librairie qui permet de lire des `CSV` (il est néanmoins
beaucoup plus pratique d'utiliser le format `parquet` avec `arrow`)


```python
from pyarrow import fs, csv

s3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')

with s3.open_input_file("donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv") as file:
  df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=";")).to_pandas()

df.head(2)
```

{{% /tab %}}

{{< /tabs >}}


## Uploader un fichier

{{< tabs tabTotal="4" >}}
{{% tab tabName="boto3" %}}


```python
s3.upload_file(file_name, bucket, object_name)
```

{{% /tab %}}

{{% tab tabName="S3FS" %}}

__TO BE COMPLETED__


{{% /tab %}}

{{% tab tabName="Arrow" %}}

__TO BE COMPLETED__

{{% /tab %}}

{{% tab tabName="Snakemake" %}}


```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier='mon_dossier_local/moninput.csv'
    output:
        fichier=S3.remote(f'{bucket}/monoutput.csv')
    run:
        import pandas as pd
        df = pd.read_csv(input.fichier)
        df.to_csv(output.fichier)
```


{{% /tab %}}

{{< /tabs >}}


## Pour aller plus loin

- [La documentation sur MinIO du SSPCloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees)
